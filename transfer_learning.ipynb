{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "australian-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -l -s git://github.com/digitalemerge/ai-sensing-ignite2020-course.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ai-sensing-ignite2020-course/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-protocol",
   "metadata": {},
   "source": [
    "Let's place all the configurations up-front. These can be modified in the future based on the dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = '/content/ai-sensing-ignite2020-course/data/train/'\n",
    "VALIDATION_DATA_DIR = '/content/ai-sensing-ignite2020-course/data/val/'\n",
    "TRAIN_SAMPLES = 500\n",
    "VALIDATION_SAMPLES = 500\n",
    "NUM_CLASSES = 2\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ultimate-stream",
   "metadata": {},
   "source": [
    "# Load and augment the data\n",
    "Colored images usually have 3 channels viz. red, green and blue, each with intensity value ranging from 0 to 255. To normalize it (i.e. bring the value between 0 to 1), we can rescale the image by dividing each pixel by 255. Or, we can use the default preprocess_input function in Keras which does the preprocessing for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-lobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   zoom_range=0.2)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-zoning",
   "metadata": {},
   "source": [
    "Time to load the data from its directories and let the augmentation happen!\n",
    "\n",
    "A few key things to note:\n",
    "\n",
    "* Training one image at a time can be pretty inefficient, so we can batch them into groups.\n",
    "* To introduce more randomness during the training process, we’ll keep shuffling the images in each batch.\n",
    "* To bring reproducibility during multiple runs of the same program, we’ll give the random number generator a seed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_generator = train_datagen.flow_from_directory(TRAIN_DATA_DIR,\n",
    "                                                    target_size=(IMG_WIDTH,\n",
    "                                                                 IMG_HEIGHT),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    shuffle=True,\n",
    "                                                    seed=12345,\n",
    "                                                    class_mode='categorical')\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    VALIDATION_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-litigation",
   "metadata": {},
   "source": [
    "Now that the data is taken care of, we come to the most crucial component of our training process - the model. We will reuse a CNN previously trained on the ImageNet dataset, remove the ImageNet specific classifier in the last few layers, and replace it with our own classifier suited to our problem. For transfer learning, we’ll ‘freeze’ the weights of the original model, i.e. set those layers as unmodifiable, so only the layers of the new classifier (that we add) can be modified. To keep things fast, we’ll choose the MobileNet model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-detroit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_maker():\n",
    "    base_model = MobileNet(include_top=False,\n",
    "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False\n",
    "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    custom_model = base_model(input)\n",
    "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
    "    custom_model = Dense(64, activation='relu')(custom_model)\n",
    "    custom_model = Dropout(0.5)(custom_model)\n",
    "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
    "    return Model(inputs=input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternative-africa",
   "metadata": {},
   "source": [
    "# Train and test\n",
    "With both the data and model ready, all we have left to do is train the model. This is also known as fitting the model to the data. For training any model, we need to pick a loss function, an optimizer, initial learning rate and a metric. Let's discuss these briefly:\n",
    "\n",
    "* Loss function: The loss function is the objective being minimized. For example, in a task to predict house prices, the loss function could be the mean squared error.\n",
    "* Optimizer: This is an optimization algorithm that helps minimize the loss function. We’ll choose Adam, one of the fastest optimizers out there.\n",
    "* Learning rate: This defines how quickly or slowly you update the weights during training. Choosing an optimal learning rate is crucial - a big value can cause the training process to jump around, missing the target. On the other hand, a tiny value can cause the training process to take ages to reach the target. We’ll keep it at 0.001 for now.\n",
    "* Metric: Choose a metric to judge the performance of the trained model. Accuracy is a good explainable metric, especially when the classes are not imbalanced, i.e. roughly equal in size. Note that this metric is not used during training to maximize or minimize an objective.\n",
    "\n",
    "You might have noticed the term epoch here. One epoch means a full training step where the network has gone over the entire dataset. One epoch may consist of several mini-batches.\n",
    "\n",
    "Run this program and let the magic begin. If you don’t have a GPU, brew a cup of coffee while you wait. You’ll notice 4 statistics - loss and accuracy on both the training and validation data. You are rooting for the val_acc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-transparency",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_maker()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=['acc'])\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=math.ceil(float(TRAIN_SAMPLES) / BATCH_SIZE),\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=math.ceil(float(VALIDATION_SAMPLES) / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-witness",
   "metadata": {},
   "source": [
    "\n",
    "On our runs, it took was 5 seconds in the very first epoch to reach 90% accuracy on the validation set, with just 500 training images. Whoa! And by the 10th step, we observe about 97% validation accuracy. That’s the power of transfer learning.\n",
    "\n",
    "Without having the model previously trained on ImageNet, getting a decent accuracy on this task would have taken (1) training time anywhere between a couple of hours to a few days (2) tons of more data to get decent results.\n",
    "\n",
    "Before we forget, save the model you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-coordinate",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-stream",
   "metadata": {},
   "source": [
    "# Model Prediction\n",
    "Now that you have a trained model, you might eventually want to use it later for your application. We can now load this model anytime and classify an image. The Keras function load_model, as the name suggests loads the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl 'https://images.theconversation.com/files/319375/original/file-20200309-118956-1cqvm6j.jpg' --output \"perrito.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-retro",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'perrito.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "preprocessed_img = expanded_img_array / 255.  # Preprocess the image\n",
    "prediction = model.predict(preprocessed_img)\n",
    "print(prediction)\n",
    "print(validation_generator.class_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
